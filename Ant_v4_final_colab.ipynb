{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!pip install mujoco\n",
        "!pip install pyvirtualdisplay\n",
        "!sudo apt-get install xvfb\n",
        "!pip install xvfbwrapper\n",
        "!pip install numpy --upgrade\n",
        "!pip install gym --upgrade\n",
        "!pip install tensorboard\n",
        "!pip uninstall dopamine-rl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9PQs3Kua2b9",
        "outputId": "414474f2-b9c1-4a71-9427-c9eb1ba3ba7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [830 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,798 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,082 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,069 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,374 kB]\n",
            "Fetched 7,496 kB in 6s (1,220 kB/s)\n",
            "Reading package lists... Done\n",
            "Collecting mujoco\n",
            "  Downloading mujoco-3.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.7.0)\n",
            "Collecting glfw (from mujoco)\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.25.2)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.11.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.18.1)\n",
            "Installing collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.7.0 mujoco-3.1.5\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 51 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.10 [28.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.10 [863 kB]\n",
            "Fetched 7,813 kB in 8s (989 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.10_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.10_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.10) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting xvfbwrapper\n",
            "  Downloading xvfbwrapper-0.2.9.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: xvfbwrapper\n",
            "  Building wheel for xvfbwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-py3-none-any.whl size=5009 sha256=4bf5b99b68969e0b87527799ce0cb50109d0d415a138462bb462f8966f6280ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/10/7d/2b7fdffccf837f7d5425931575fbee9caebe2c190931f9058b\n",
            "Successfully built xvfbwrapper\n",
            "Installing collected packages: xvfbwrapper\n",
            "Successfully installed xvfbwrapper-0.2.9\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "Successfully installed numpy-1.26.4\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting gym\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827623 sha256=c2e982d4492c481c40169ca5e4242161eecc1ee7777d93432b4ecc29d7357ccf\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.63.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Found existing installation: dopamine_rl 4.0.9\n",
            "Uninstalling dopamine_rl-4.0.9:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/dopamine/*\n",
            "    /usr/local/lib/python3.10/dist-packages/dopamine_rl-4.0.9.dist-info/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled dopamine_rl-4.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# 가상 디스플레이를 사용해 화면 표시\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# 비디오 녹화를 위한 함수 정의\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")"
      ],
      "metadata": {
        "id": "rLnoOMyKa7go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 정의\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=256):\n",
        "        super(Actor, self).__init__()\n",
        "        self.neural_net = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.from_numpy(x).to(device)\n",
        "        return self.neural_net(x.float())\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=256):\n",
        "        super(Critic, self).__init__()\n",
        "        self.neural_net = nn.Sequential(\n",
        "            nn.Linear(state_size + action_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).to(device)\n",
        "        if isinstance(action, np.ndarray):\n",
        "            action = torch.from_numpy(action).to(device)\n",
        "        x = torch.hstack((state, action))\n",
        "        return self.neural_net(x.float())"
      ],
      "metadata": {
        "id": "To-LRP8NiLmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리플레이 버퍼 정의\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((np.array(state), np.array(action), reward, np.array(next_state), done))\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = map(np.array, zip(*experiences))\n",
        "        return (\n",
        "            torch.from_numpy(states).float(),\n",
        "            torch.from_numpy(actions).float(),\n",
        "            torch.from_numpy(rewards).float(),\n",
        "            torch.from_numpy(next_states).float(),\n",
        "            torch.from_numpy(dones).int()\n",
        "        )"
      ],
      "metadata": {
        "id": "Kyso6mvCiLXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DDPG 에이전트 정의\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_size, action_size, action_low, action_high,\n",
        "                 actor_lr=1e-4, critic_lr=1e-3, buffer_size=100000, batch_size=400,\n",
        "                 gamma=0.99, tau=1e-3, noise_scale=0.5,\n",
        "                 initial_std=1.0, min_std=0.1, decay_rate=0.01):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_low = action_low\n",
        "        self.action_high = action_high\n",
        "\n",
        "        # 액터 및 크리틱 네트워크 초기화\n",
        "        self.actor = Actor(state_size, action_size).to(device)\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.target_actor = Actor(state_size, action_size).to(device)\n",
        "        self.target_critic = Critic(state_size, action_size).to(device)\n",
        "\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        # 최적화 및 버퍼\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size, batch_size)\n",
        "\n",
        "        # 파라미터\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "        self.noise_scale = noise_scale\n",
        "        self.current_std = initial_std\n",
        "        self.min_std = min_std\n",
        "        self.decay_rate = decay_rate\n",
        "\n",
        "    def update_std(self, episode_num):\n",
        "        self.current_std = max(self.min_std, self.current_std * np.exp(-self.decay_rate * episode_num))\n",
        "\n",
        "    def act(self, state):\n",
        "        if not isinstance(state, np.ndarray):\n",
        "            state = np.array(state)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # state.shape: torch, [1,27]]\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state).cpu().numpy().squeeze()  # action.shape: np, [8,]\n",
        "        self.actor.train()\n",
        "\n",
        "        noise = self.noise_scale * (np.random.randn(self.action_size) * self.current_std)  # noise.shape: np, [8,]\n",
        "        action = np.clip(action + noise, self.action_low, self.action_high)\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) < self.replay_buffer.batch_size:\n",
        "            return None\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample()  # .shape: torch.float, [64,27][64,8][64,][64,27][64,]\n",
        "\n",
        "        states = states.to(device)\n",
        "        actions = actions.to(device)\n",
        "        rewards = rewards.unsqueeze(1).to(device)\n",
        "        next_states = next_states.to(device)\n",
        "        dones = dones.unsqueeze(1).to(device)\n",
        "\n",
        "        # 타깃 액터와 크리틱의 계산\n",
        "        next_actions = self.target_actor(next_states)  # [64,8]\n",
        "        target_q_values = self.target_critic(next_states, next_actions)  # [64,1]\n",
        "        q_targets = rewards + (self.gamma * target_q_values * (1 - dones))  # [64,1]\n",
        "\n",
        "        # 크리틱 업데이트\n",
        "        q_expected = self.critic(states, actions)  # [64,1]\n",
        "        critic_loss = F.mse_loss(q_expected, q_targets)\n",
        "        critic_log = critic_loss.item()\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # 액터 업데이트\n",
        "        predicted_actions = self.actor(states)  # [64,8]\n",
        "        actor_loss = - self.critic(states, predicted_actions).mean()\n",
        "        actor_log = actor_loss.item()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # 타깃 네트워크 업데이트\n",
        "        with torch.no_grad():\n",
        "            for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "            for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "        return (critic_log, actor_log)"
      ],
      "metadata": {
        "id": "WqgEbtwgCFnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 및 DDPG 에이전트 초기화\n",
        "env_name = 'Ant-v4'\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, './video', episode_trigger=lambda episode_number: (episode_number + 1) % 33 == 0)\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.shape[0]\n",
        "action_low = float(env.action_space.low[0])\n",
        "action_high = float(env.action_space.high[0])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "agent = DDPGAgent(state_size, action_size, action_low, action_high,\n",
        "                  actor_lr=1e-4, critic_lr=1e-3, buffer_size=100000, batch_size=64,\n",
        "                  gamma=0.99, tau=1e-3, noise_scale=0.1,\n",
        "                  initial_std=1.0, min_std=0.1, decay_rate=0.01\n",
        "                  )"
      ],
      "metadata": {
        "id": "J7W-lg36iLKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d564bb-9ad7-4442-dabf-092f0eb231a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/Colab_Notebooks/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프\n",
        "num_episodes = 10000\n",
        "checkpoint_path = './ddpg_checkpoint.pth'\n",
        "save_interval = 33\n",
        "\n",
        "global_total_rewards = []\n",
        "q_losses = []\n",
        "policy_losses = []\n",
        "mean_q_losses = []\n",
        "mean_policy_losses = []\n",
        "\n",
        "# 필요한 경우 이전에 저장된 체크포인트에서 상태를 불러옴\n",
        "try:\n",
        "    agent.load_checkpoint(checkpoint_path)\n",
        "    print(\"Checkpoint loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"No checkpoint found or failed to load, starting fresh.\")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    q_losses = []\n",
        "    policy_losses = []\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # 학습\n",
        "        log = agent.learn()\n",
        "        if log is not None:\n",
        "            q_loss, policy_loss = log\n",
        "            q_losses.append(q_loss)\n",
        "            policy_losses.append(policy_loss)\n",
        "\n",
        "    global_total_rewards.append(total_reward)\n",
        "    mean_q_losses.append(np.array(q_losses).mean())\n",
        "    mean_policy_losses.append(np.array(policy_losses).mean())\n",
        "\n",
        "    agent.update_std(episode)  # 각 에피소드마다 표준 편차 갱신\n",
        "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    # 일정한 에피소드마다 체크포인트를 저장\n",
        "    if (episode + 1) % save_interval == 0:\n",
        "        agent.save_checkpoint(checkpoint_path)\n",
        "        print(f\"Checkpoint saved at episode {episode + 1}.\")\n",
        "\n",
        "# 환경 종료\n",
        "env.close()\n",
        "show.video()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(mean_q_losses, label='Q-Loss')\n",
        "plt.plot(mean_policy_losses, label='Policy Loss')\n",
        "plt.plot(global_total_rewards, label='Total Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Losses & Rewards Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XMrOpcLcih9h",
        "outputId": "3957b711-ca29-4c8b-d5e5-0dd71c708a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = -40.13842351942996\n",
            "Episode 2: Total Reward = -220.68893227341096\n",
            "Episode 3: Total Reward = -56.08848181070197\n",
            "Episode 4: Total Reward = -31.48094640820867\n",
            "Episode 5: Total Reward = -75.11191497805379\n",
            "Episode 6: Total Reward = -62.88342347094091\n",
            "Episode 7: Total Reward = -97.52615203911378\n",
            "Episode 8: Total Reward = -128.0798960499395\n",
            "Episode 9: Total Reward = -26.80444793563153\n",
            "Episode 10: Total Reward = -81.79983212151006\n",
            "Episode 11: Total Reward = -47.1381048500408\n",
            "Episode 12: Total Reward = -851.1588498001141\n",
            "Episode 13: Total Reward = -95.31579684537184\n",
            "Episode 14: Total Reward = -461.9516456988403\n",
            "Episode 15: Total Reward = -373.6428705585123\n",
            "Episode 16: Total Reward = -133.94393052241722\n",
            "Episode 17: Total Reward = -1621.1145321696365\n",
            "Episode 18: Total Reward = -90.04993089700591\n",
            "Episode 19: Total Reward = -136.02507378571093\n",
            "Episode 20: Total Reward = -33.883999825961354\n",
            "Episode 21: Total Reward = -179.23358388323172\n",
            "Episode 22: Total Reward = -131.11952344343476\n",
            "Episode 23: Total Reward = -30.582123407637745\n",
            "Episode 24: Total Reward = -64.39356915282511\n",
            "Episode 25: Total Reward = -849.4249572441761\n",
            "Episode 26: Total Reward = -53.736464365147995\n",
            "Episode 27: Total Reward = -192.39434904636175\n",
            "Episode 28: Total Reward = -104.00048427545607\n",
            "Episode 29: Total Reward = -264.92003283214706\n",
            "Episode 30: Total Reward = -41.286256145569254\n",
            "Checkpoint saved at episode 30.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-48ff4ccb662b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mq_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0f828ae56a63>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# 액터 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mpredicted_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1032859d85bf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, noise_scale)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmu\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# env_name = 'Ant-v4'\n",
        "# env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "# env = RecordVideo(env, './video', episode_trigger=lambda episode_number: True)\n",
        "# state = env.reset()\n",
        "\n",
        "# while True:\n",
        "\n",
        "#   action = env.action_space.sample()\n",
        "#   next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "#   if terminated or truncated: break\n",
        "\n",
        "# env.close()\n",
        "# show_video()"
      ],
      "metadata": {
        "id": "7m-qvlG6dQu3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}