{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!pip install mujoco\n",
        "!pip install pyvirtualdisplay\n",
        "!sudo apt-get install xvfb\n",
        "!pip install xvfbwrapper\n",
        "!pip install numpy --upgrade\n",
        "!pip install gym --upgrade\n",
        "!pip install tqdm\n",
        "!pip uninstall dopamine-rl"
      ],
      "metadata": {
        "id": "w9PQs3Kua2b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "from pyvirtualdisplay import Display\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 가상 디스플레이를 사용해 화면 표시\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# 비디오 녹화를 위한 함수 정의\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")"
      ],
      "metadata": {
        "id": "rLnoOMyKa7go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 정의\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=256):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return torch.tanh(self.fc3(x))\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=256):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size + action_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "To-LRP8NiLmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리플레이 버퍼 정의\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((np.array(state), np.array(action), reward, np.array(next_state), done))\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards)\n",
        "        next_states = np.array(next_states)\n",
        "        dones = np.array(dones)\n",
        "\n",
        "        return (\n",
        "            torch.tensor(states, dtype=torch.float),\n",
        "            torch.tensor(actions, dtype=torch.float),\n",
        "            torch.tensor(rewards, dtype=torch.float),\n",
        "            torch.tensor(next_states, dtype=torch.float),\n",
        "            torch.tensor(dones, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "Kyso6mvCiLXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DDPG 에이전트 정의\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_size, action_size, action_low, action_high):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_low = action_low\n",
        "        self.action_high = action_high\n",
        "\n",
        "        # 액터 및 크리틱 네트워크 초기화\n",
        "        self.actor = Actor(state_size, action_size).to(device)\n",
        "        self.critic = Critic(state_size, action_size).to(device)\n",
        "        self.target_actor = Actor(state_size, action_size).to(device)\n",
        "        self.target_critic = Critic(state_size, action_size).to(device)\n",
        "\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "        # 최적화 및 버퍼\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "        self.replay_buffer = ReplayBuffer(100000, 64)\n",
        "\n",
        "        # 파라미터\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 1e-3\n",
        "\n",
        "    def act(self, state, noise_scale=0.1):\n",
        "        state = np.array(state)\n",
        "        state = torch.tensor(state, dtype=torch.float).to(device).unsqueeze(0)\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor(state).cpu().numpy().squeeze()\n",
        "        self.actor.train()\n",
        "\n",
        "        noise = noise_scale * np.random.randn(self.action_size)\n",
        "        action = np.clip(action + noise, self.action_low, self.action_high)\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) < self.replay_buffer.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample()\n",
        "\n",
        "        states = states.to(device)\n",
        "        actions = actions.to(device)\n",
        "        rewards = rewards.to(device)\n",
        "        next_states = next_states.to(device)\n",
        "        dones = dones.to(device)\n",
        "\n",
        "        # 타깃 액터와 크리틱의 계산\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.target_actor(next_states)\n",
        "            target_q_values = self.target_critic(next_states, next_actions)\n",
        "            q_targets = rewards + (self.gamma * target_q_values.squeeze() * (1 - dones))\n",
        "\n",
        "        # 크리틱 업데이트\n",
        "        q_expected = self.critic(states, actions).squeeze()\n",
        "        critic_loss = nn.MSELoss()(q_expected, q_targets)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # 액터 업데이트\n",
        "        predicted_actions = self.actor(states)\n",
        "        actor_loss = -self.critic(states, predicted_actions).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # 타깃 네트워크 업데이트\n",
        "        with torch.no_grad():\n",
        "            for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "            for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "    def save_checkpoint(self, filepath):\n",
        "        checkpoint = {\n",
        "            'actor_state_dict': self.actor.state_dict(),\n",
        "            'critic_state_dict': self.critic.state_dict(),\n",
        "            'target_actor_state_dict': self.target_actor.state_dict(),\n",
        "            'target_critic_state_dict': self.target_critic.state_dict(),\n",
        "            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),\n",
        "            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),\n",
        "            'replay_buffer': list(self.replay_buffer.memory),\n",
        "            'gamma': self.gamma,\n",
        "            'tau': self.tau\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "\n",
        "    def load_checkpoint(self, filepath):\n",
        "        checkpoint = torch.load(filepath)\n",
        "        self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
        "        self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
        "        self.target_actor.load_state_dict(checkpoint['target_actor_state_dict'])\n",
        "        self.target_critic.load_state_dict(checkpoint['target_critic_state_dict'])\n",
        "        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
        "        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
        "        self.replay_buffer.memory = deque(checkpoint['replay_buffer'], maxlen=100000)\n",
        "        self.gamma = checkpoint['gamma']\n",
        "        self.tau = checkpoint['tau']"
      ],
      "metadata": {
        "id": "WqgEbtwgCFnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 및 DDPG 에이전트 초기화\n",
        "env_name = 'Ant-v4'\n",
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, '/content/drive/MyDrive/Colab_Notebooks/video', episode_trigger=lambda episode_number: (episode_number+1) % 100 == 0)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.shape[0]\n",
        "action_low = float(env.action_space.low[0])\n",
        "action_high = float(env.action_space.high[0])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "agent = DDPGAgent(state_size, action_size, action_low, action_high)"
      ],
      "metadata": {
        "id": "J7W-lg36iLKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fadb69e2-69fb-4d35-f293-e39b681b3813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/MyDrive/Colab_Notebooks/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프 수정: 진행 중간에 체크포인트를 저장하고 불러오는 기능 추가\n",
        "num_episodes = 10000\n",
        "checkpoint_path = '/content/drive/MyDrive/Colab_Notebooks/ddpg_checkpoint.pth'\n",
        "save_interval = 30\n",
        "\n",
        "# 필요한 경우 이전에 저장된 체크포인트에서 상태를 불러옴\n",
        "try:\n",
        "    agent.load_checkpoint(checkpoint_path)\n",
        "    print(\"Checkpoint loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"No checkpoint found or failed to load, starting fresh.\")\n",
        "\n",
        "# 학습 루프\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        # 학습\n",
        "        agent.learn()\n",
        "\n",
        "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "    # 일정한 에피소드마다 체크포인트를 저장\n",
        "    if (episode + 1) % save_interval == 0:\n",
        "        agent.save_checkpoint(checkpoint_path)\n",
        "        print(f\"Checkpoint saved at episode {episode + 1}.\")\n",
        "\n",
        "# 환경 종료 및 녹화된 비디오 표시\n",
        "env.close()\n",
        "show_video()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XMrOpcLcih9h",
        "outputId": "0775d6db-7870-4a2c-e15d-18e899fd4e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found or failed to load, starting fresh.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/drive/MyDrive/Colab_Notebooks/video/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/drive/MyDrive/Colab_Notebooks/video/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/drive/MyDrive/Colab_Notebooks/video/rl-video-episode-0.mp4\n",
            "Episode 1: Total Reward = 254.69029171039648\n",
            "Episode 2: Total Reward = 14.03197935030156\n",
            "Episode 3: Total Reward = 170.40009316049512\n",
            "Episode 4: Total Reward = 820.338534296169\n",
            "Episode 5: Total Reward = 857.8765992796526\n",
            "Episode 6: Total Reward = 849.8298694014949\n",
            "Episode 7: Total Reward = 877.1542807485616\n",
            "Episode 8: Total Reward = 820.1857162870518\n",
            "Episode 9: Total Reward = 856.9519369037855\n",
            "Episode 10: Total Reward = 788.3395037404863\n",
            "Episode 11: Total Reward = 838.054600455282\n",
            "Episode 12: Total Reward = 838.022500628624\n",
            "Episode 13: Total Reward = 799.7251858975787\n",
            "Episode 14: Total Reward = 883.927178137473\n",
            "Episode 15: Total Reward = 878.7315275467703\n",
            "Episode 16: Total Reward = 900.1660709823581\n",
            "Episode 17: Total Reward = 702.680805213414\n",
            "Episode 18: Total Reward = 665.2903578380531\n",
            "Episode 19: Total Reward = -5.053638067629159\n",
            "Episode 20: Total Reward = 799.0480514758798\n",
            "Episode 21: Total Reward = 723.809661711679\n",
            "Episode 22: Total Reward = 658.872018991885\n",
            "Episode 23: Total Reward = 762.1649303386075\n",
            "Episode 24: Total Reward = 726.2085755975131\n",
            "Episode 25: Total Reward = 571.3162350274948\n",
            "Episode 26: Total Reward = 781.1806642238894\n",
            "Episode 27: Total Reward = 747.662322838215\n",
            "Episode 28: Total Reward = 709.5902474079269\n",
            "Episode 29: Total Reward = 664.7312065834074\n",
            "Episode 30: Total Reward = 453.8885130884742\n",
            "Checkpoint saved at episode 30.\n",
            "Episode 31: Total Reward = 603.1741316655196\n",
            "Episode 32: Total Reward = 675.8588042651489\n",
            "Episode 33: Total Reward = 703.3276487745527\n",
            "Episode 34: Total Reward = 648.0742974033187\n",
            "Episode 35: Total Reward = 659.9146155273168\n",
            "Episode 36: Total Reward = 657.0645246304334\n",
            "Episode 37: Total Reward = 563.908164160586\n",
            "Episode 38: Total Reward = 695.6446710914472\n",
            "Episode 39: Total Reward = 735.0279739166737\n",
            "Episode 40: Total Reward = 736.952270468705\n",
            "Episode 41: Total Reward = 748.9821291844938\n",
            "Episode 42: Total Reward = 726.2130574564969\n",
            "Episode 43: Total Reward = 720.0192651126372\n",
            "Episode 44: Total Reward = 496.7053686571702\n",
            "Episode 45: Total Reward = 698.7456015075625\n",
            "Episode 46: Total Reward = 31.70479853994563\n",
            "Episode 47: Total Reward = 247.07860074215785\n",
            "Episode 48: Total Reward = 76.23091268226678\n",
            "Episode 49: Total Reward = 29.31134356400196\n",
            "Episode 50: Total Reward = 686.594722066237\n",
            "Episode 51: Total Reward = 676.4463205461433\n",
            "Episode 52: Total Reward = 312.4280854449528\n",
            "Episode 53: Total Reward = 561.6465875171647\n",
            "Episode 54: Total Reward = 611.203733410292\n",
            "Episode 55: Total Reward = 432.0327331686405\n",
            "Episode 56: Total Reward = 49.450820038850416\n",
            "Episode 57: Total Reward = 591.012874476559\n",
            "Episode 58: Total Reward = 551.3287974815988\n",
            "Episode 59: Total Reward = 639.7650014645537\n",
            "Episode 60: Total Reward = 139.3696405889222\n",
            "Checkpoint saved at episode 60.\n",
            "Episode 61: Total Reward = 626.679083675502\n",
            "Episode 62: Total Reward = 526.7509091938922\n",
            "Episode 63: Total Reward = 711.5152304457391\n",
            "Episode 64: Total Reward = 499.47266997236636\n",
            "Episode 65: Total Reward = 612.6059027136407\n",
            "Episode 66: Total Reward = 718.7742295183174\n",
            "Episode 67: Total Reward = 549.6958283340846\n",
            "Episode 68: Total Reward = 23.645450436620074\n",
            "Episode 69: Total Reward = 112.1358078803104\n",
            "Episode 70: Total Reward = 800.5474491689521\n",
            "Episode 71: Total Reward = -213.8333414902676\n",
            "Episode 72: Total Reward = 553.8299212440968\n",
            "Episode 73: Total Reward = 616.380470835792\n",
            "Episode 74: Total Reward = 180.00236135704597\n",
            "Episode 75: Total Reward = 33.99054036313115\n",
            "Episode 76: Total Reward = -1.0776113256159359\n",
            "Episode 77: Total Reward = 97.41511240161621\n",
            "Episode 78: Total Reward = 529.125387252999\n",
            "Episode 79: Total Reward = 47.5563659802092\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e1dd24562244>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Episode {episode + 1}: Total Reward = {total_reward}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c51a8b7059de>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# 타깃 네트워크 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m                             )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 state_steps)\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    317\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mstep_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# env_name = 'Ant-v4'\n",
        "# env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "# env = RecordVideo(env, './video', episode_trigger=lambda episode_number: True)\n",
        "# state = env.reset()\n",
        "\n",
        "# while True:\n",
        "\n",
        "#   action = env.action_space.sample()\n",
        "#   next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "#   if terminated or truncated: break\n",
        "\n",
        "# env.close()\n",
        "# show_video()"
      ],
      "metadata": {
        "id": "7m-qvlG6dQu3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}